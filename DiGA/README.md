## Codes for Diffusion Guided Meta Agent~(DiGA)

### Introduction
In this repository, we provide the core codes of [DiGA](https://arxiv.org/pdf/2408.12991), including training and generating scripts.
`train.py` is the Python script used for training meta controller. `generate.py` is the Python script used for generating meta agent that is guided by the meta controller for obtaining synthetic market trading records. The code for training rl agent in the simulated environment is provided under `rltask/train_test_rl.py` script.

### Prerequisites
We recommend to use conda environment. The required packages can be install with:
```python
conda env create --file environment.yml
```
After installation, use `conda activate diga` for activating the conda environment.

After that, install required packages following the instructions from [MarS](https://github.com/microsoft/MarS).

Data should be processed into the shape of $(N, C, T)$ where $N$ is the number of samples, $C$ is the number of market state parameters, $T$ is the effective number of minutes of each sample. In our case, $C=2$ along where the first dimension should be the mid-price log return rate and the second dimension should be the number of orders within each minute; $T=236$ which is the effective number of minutes in one trading day. 

> Original data can be purchased from lisenced data vendors (e.g. Wind, Thomson Reuters).


## Code Examples
For training a meta controller:

```python
python train.py --data_name "SZAMain" --ctrl_type "continuous" --ctrl_target "return" --n_bins 5 --diffsteps 200 --epochs 10 --checkpoints 3 --data_path {your_data_path} --output_path {your_output_path} --seed 0
```

Code above will train meta controller with 'SZAMain' dataset, control on return, using continuous control encoder. By default the control target(return) is divided into 5 bins (indicating 5 control classes: lower, low, medium, high, higher). Moreover, the diffusion model in meta controller is configured to perform 200 diffusion steps, trained with a maximum number of epochs as 10. Make sure the dataset is stored in `{your_data_path}`, named after `{data_name}_train.npy` and `{data_name}_vali.npy`. The trained model will be saved inside `{your_output_path}/DiGA_{data_name}_{ctrl_type}_{ctrl_target}_{seed}/` by default. 

For generating with meta agent guided by meta controller:

```python
python generate.py --data_name "SZAMain" --ctrl_type "continuous" --ctrl_target "return" --ctrl_class 0 --cond_scale 1 --samsteps 20 --data_path {your_data_path} --output_path {your_output_path} --exp_name {your_exp_name} --checkpoint_path "last.ckpt" --save_name "DiGA_generation" --seed 0
```

Code above will first sample from the trained meta controller, conditioned on `ctrl_class` of `ctrl_target`. If `ctrl_type` is "discrete", then the `ctrl_class` refers to the selected bin. If `ctrl_type` is "continuous", the `ctrl_class` refers to the relative strength of `ctrl_target`. `cond_scale` controls the strength of classifier-free guidance during diffusion model sampling. After sampling, the meta agent will generate the trade records within one trading day and the records are saved into `{output_path}/{exp_name}/{save_name}.pkl`, where the `exp_name` may be `"DiGA_{data_name}_{ctrl_type}_{ctrl_target}_{seed}"` in the above sample.

For running RL training in the generated market:
```python
python rltask/train_test_rl.py --market "DiGA" --data_path {your_data_path} --test_replay_path {your_test_replay_path} --output_path {your_output_path} --save_name {your_save_name}
```
Code above will train RL agent in a market environment generated by DiGA. It takes pre-computed meta controller samples from `data_path` for better efficiency. The trained agent and testing results are stored in `{output_path}/{save_name}`. For training with DiGA environment, the file in `data_path` should contain a dict with each item storing one sample generated by meta controller. For training (or testing) with Replay environment, the data in `data_path` (or `test_replay_path`) should contain paths to orders and transactions records (both in csv, preprocessed using market_simulation libary).

### Code argument details
`train.py` accepts the following arguments:
- `--data_name`: The name of the dataset to use for training.
- `--ctrl_type`: The type of control to use (continuous or discrete).
- `--ctrl_target`: The target of the control. (i.e. return, volatility)
- `--n_bins`: The number of bins to use for the discretization of `ctrl_target`.
- `--diffsteps`: The number of diffusion steps.
- `--samsteps`: The number of sampling steps.
- `--epochs`: The number of training epochs. Either `epochs` or `maxsteps` should be set and the training will stop when either one is reached.
- `--maxsteps`: The maximum number of steps for the trainer. Either `epochs` or `maxsteps` should be set and the training will stop when either one is reached.
- `--batch_size`: The batch size for training.
- `--learning_rate`: The learning rate for the optimizer.
- `--checkpoints`: The number of checkpoints to save.
- `--data_path`: The path to your training data.
- `--output_path`: The path where you want to save your trained model and other output files.
- `--seed`: The seed for random number generation.
- `--num_workers`: The number of workers to use for data loading.

`generate.py` accepts the following arguments:

- `--data_name`: The name of the dataset to use for training.
- `--ctrl_type`: The type of control to use (continuous or discrete).
- `--ctrl_target`: The target of the control. (i.e. return, volatility)
- `--n_bins`: The number of bins to use for the discretization of `ctrl_target`.
- `--diffsteps`: The number of diffusion steps.
- `--samsteps`: The number of sampling steps.
- `--seed`: The seed for random number generation.
- `--data_path`: The path to your training data.
- `--output_path`: The path where you want to save your trained model and other output files.
- `--exp_name`: The name of the experiment.
- `--checkpoint_path`: The path to the checkpoint file from your `output_path`.
- `--save_name`: The name of the file to save the generated data.
- `--random_price`: Whether to generate a random initial price.
- `--pseudo_price`: The initial price to use if random_price is not set.

`rltask/train_test_rl.py` accepts the following arguments:

- `--market`: The type of market environment to use for training. Options are 'DiGA' and 'Replay'.
- `--max_steps`: The maximum number of steps for the trainer.
- `--save_name`: The folder name for saving rl run.
- `--eval_eps`: The number of evaluation episodes.
- `--data_path`: The path to data for generating training environment.
- `--test_replay_path`: The path to data for generating testing environment.
- `--output_path`: The path where you want to save your trained model and other output files.
- `--seed`: The seed for random number generation.