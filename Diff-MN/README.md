# Diff-MN

Time series generation (TSG) is widely used across domains, yet most existing methods assume regular sampling and fixed output resolutions. These assumptions are often violated in practice, where observations are irregular and sparse, while downstream applications require continuous and high-resolution TS. 

Although Neural Controlled Differential Equation (NCDE) is promising for modeling irregular TS, it is constrained by a single dynamics function, tightly coupled optimization, and limited ability to adapt learned dynamics to newly generated samples from the generative model.

We propose Diff-MN, a continuous TSG framework that enhances NCDE with a Mixture-of-Experts (MoE) dynamics function and a decoupled architectural design for dynamics-focused training. 

To further enable NCDE to generalize to newly generated samples, Diff-MN employs a diffusion model to parameterize the NCDE temporal dynamics parameters (MoE weights), i.e.,
jointly learn the distribution of TS data and MoE weights. This design allows sample-specific NCDE parameters to be generated for continuous TS generation. 

Experiments on ten public and synthetic datasets demonstrate that Diff-MN consistently outperforms strong baselines on both irregular-to-regular and irregular-to-continuous TSG tasks. 



## Environment

Install the environment using the YAML file: `./environment_diffmn.yml`:

```bash
conda env create -f environment_diffmn.yml --force --no-deps
```

## Data
Stocks and Energy data are located in `./datasets`. Sine, MuJoCo, polynomial datasets are generated and the scripts are included in `./datasets` folder.

`utils_data.py` provides functions for loading data in both regular and irregular settings. In particular, irregular data are preprocessed using the Python class `TimeDataset_irregular`, which may take some time to run. Once preprocessing is complete, the processed data are saved in the `./datasets` directory for future use.


## Reproducing the paper results
By setting the time series length and missing values within the script, the results in the paper can be reproduced:


**Step 1:** The initial MoE NeuralCDE can be trained by `run_irregular_moencde.py`.

---

**Step 2:** Parameterizing the MoE weights can be achieved by jointly training the TS samples and their corresponding MoE weights through script `run_diffmn_diffsuion.py`.

---

**Step 3:** Through Step 2, we generate new samples along with their corresponding MoE weights. These weights are then fed into the pretrained MoE Neural CDE to perform continuous time series generation for each new sample. Finally, the refined high-frequency continuous time series are obtained using the script `run_irregular_moencde_continues.py`, providing richer temporal information and improving the accuracy of downstream tasks.
